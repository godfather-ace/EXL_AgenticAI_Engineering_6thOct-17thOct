{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86076c98",
   "metadata": {},
   "source": [
    "## Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc0b79",
   "metadata": {},
   "source": [
    "Evaluating an agent requires a thorough check at the agent's entire lifecycle, from development to deployment. Some important questions need to be addressed: \n",
    "\n",
    "* **Final output:** Is the agent responding in a factual and genuine manner? \n",
    "* **Reasoning Process:** Is the agent using correct tool and following proper logic with efficiency to reach the solution? \n",
    "* **Structural Integrity:** Are the generated responses precise, structured (eg. JSON), and reliable with regards to the tools and APIs? \n",
    "* **Conversational Skill:** Is the generated response realistic, multi-turn dialog without losing the context or getting confused? \n",
    "* **Live Feedback:** Is the quality of agent holding up over time with real, unpredictable user traffic, and can we monitor it to catch errors? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073c60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "# pip install langchain langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1206f15",
   "metadata": {},
   "source": [
    "### Initialise the APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90eed41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import langsmith\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d69ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the LangSmith client\n",
    "client = langsmith.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54659ec3",
   "metadata": {},
   "source": [
    "### 1. Exact-Match based Evaluation\n",
    "\n",
    "It is one of the fundamental evaluation methods, where we check if the model's output is identical to a predefined correct answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ce528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a dataset which will serve as a container for our Q-A examples. \n",
    "ds = client.create_dataset(\n",
    "    dataset_name = \"exact_match_dataset\", \n",
    "    description = \"A dataset for simple exact match evaluation\"\n",
    ")\n",
    "\n",
    "# After creating the dataset, we need to put the examples which is the input/output dictionary \n",
    "# The inputs and outputs are provided in separate lists, maintaining the same order. \n",
    "client.create_examples(\n",
    "    # List of inputs where each input is a dictionary\n",
    "    inputs = [\n",
    "        {\n",
    "            \"prompt_template\": \"When was artificial intelligence word coined?\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt_template\": \"When did ChatGPT launch?\"\n",
    "        }\n",
    "    ],\n",
    "    outputs = [\n",
    "        {\"output\": \"1956\"},\n",
    "        {\"output\": \"2022\"}\n",
    "    ],\n",
    "    dataset_id = ds.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb527cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that you need to test\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "def predict_results(input_: dict) -> dict:\n",
    "    # The input dictionary will have the key prompt_template which matches the key in dataset's input\n",
    "    prompt = input_[\"prompt_template\"]\n",
    "    # Initialise and call the OpenAI model\n",
    "    response = ChatOpenAI(model = model, temperature = 0).invoke(prompt)\n",
    "    # The output key \"output\" needs to match with the dataset's output\n",
    "    return {\"output\": response.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9314c",
   "metadata": {},
   "source": [
    "#### Building our custom evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afaec7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "\n",
    "@run_evaluator\n",
    "def compare_label(run, example) -> EvaluationResult: \n",
    "    \"\"\"\n",
    "    A custom evaluator that checks for an exact match\n",
    "    \n",
    "    Args: \n",
    "        run: The LangSmith run object, which contains the model's outputs.\n",
    "        example: The LangSmith example object, which contains the reference data. \n",
    "    \n",
    "    Returns: \n",
    "        An EvaluationResult object with a key and a score. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the model's prediction from run's output dictionary\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    # Get the reference answer from the example's output dictionary\n",
    "    target = example.outputs.get(\"output\") or \"\"\n",
    "    # Perform the comparison\n",
    "    match = prediction == target\n",
    "    # Return the result \n",
    "    return EvaluationResult(key = \"matches_label\", score = int(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5bd37c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'puzzled-train-95' at:\n",
      "https://smith.langchain.com/datasets/592d850b-827d-444f-8586-4fb434f6dcc8/compare?selectedSessions=7eebe4d3-c3d5-4c16-a0d2-32cff635b9f1\n",
      "\n",
      "View all tests for Dataset exact_match_dataset at:\n",
      "https://smith.langchain.com/datasets/592d850b-827d-444f-8586-4fb434f6dcc8\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.exact_match</th>\n",
       "      <th>feedback.matches_label</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c595abad-b204-4248-aa22-c8eb706378f8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.040328</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.366433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.781221</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.910775</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.040328</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.169881</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.299435</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.exact_match  feedback.matches_label error  execution_time  \\\n",
       "count                    2.0                     2.0     0        2.000000   \n",
       "unique                   NaN                     NaN     0             NaN   \n",
       "top                      NaN                     NaN   NaN             NaN   \n",
       "freq                     NaN                     NaN   NaN             NaN   \n",
       "mean                     0.0                     0.0   NaN        2.040328   \n",
       "std                      0.0                     0.0   NaN        0.366433   \n",
       "min                      0.0                     0.0   NaN        1.781221   \n",
       "25%                      0.0                     0.0   NaN        1.910775   \n",
       "50%                      0.0                     0.0   NaN        2.040328   \n",
       "75%                      0.0                     0.0   NaN        2.169881   \n",
       "max                      0.0                     0.0   NaN        2.299435   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      2  \n",
       "unique                                     2  \n",
       "top     c595abad-b204-4248-aa22-c8eb706378f8  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'puzzled-train-95',\n",
       " 'results': {'c6ff06c9-fe9e-4ef6-9678-602f3361ace6': {'input': {'prompt_template': 'When did ChatGPT launch?'},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=0, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6682dedd-aee8-4e8c-92dc-582d18ecbab0'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None),\n",
       "    EvaluationResult(key='matches_label', score=0, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('651cda88-f5c6-4fc4-a46d-7dc9ed67c051'), target_run_id=None, extra=None)],\n",
       "   'execution_time': 1.781221,\n",
       "   'run_id': 'c595abad-b204-4248-aa22-c8eb706378f8',\n",
       "   'output': {'output': 'ChatGPT was launched by OpenAI in November 2022.'},\n",
       "   'reference': {'output': '2022'}},\n",
       "  'd9905c96-54be-4f88-943f-16f03fe6ea75': {'input': {'prompt_template': 'When was artificial intelligence word coined?'},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=0, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c4263e2b-197c-48ff-9a40-041b96dfe617'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None),\n",
       "    EvaluationResult(key='matches_label', score=0, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('0e5cf05a-a508-4f5c-96d9-c215cca3412d'), target_run_id=None, extra=None)],\n",
       "   'execution_time': 2.299435,\n",
       "   'run_id': '04d401ee-15bc-4880-b9f9-a73295a6df82',\n",
       "   'output': {'output': 'The term \"artificial intelligence\" was coined in 1956 by John McCarthy, a computer scientist, during the Dartmouth Conference. This event is considered the founding moment of AI as a field of study.'},\n",
       "   'reference': {'output': '1956'}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators = [\"exact_match\"],\n",
    "    custom_evaluators = [compare_label]\n",
    ")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name = \"exact_match_dataset\",\n",
    "    llm_or_chain_factory = predict_results, \n",
    "    evaluation = eval_config, \n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b769b",
   "metadata": {},
   "source": [
    "### Unstructured QA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "150e9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset in LangSmith\n",
    "ds1 = client.create_dataset(\n",
    "    dataset_name = \"unstruct_qa_evals\",\n",
    "    description = \"Q&A dataset about LangSmith documentation.\"\n",
    ")\n",
    "\n",
    "# These are our question-and-answer examples. The answers serve as 'ground truth'.\n",
    "qa_examples = [\n",
    "    (\n",
    "        \"What is LangChain?\",\n",
    "        \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What's a langsmith dataset?\",\n",
    "        \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Add the examples to our dataset\n",
    "# The input key is 'question' and the output key is 'answer'.\n",
    "# These keys must match what our RAG chain expects and produces.\n",
    "for question, answer in qa_examples:\n",
    "    client.create_example(\n",
    "        inputs = {\"question\": question},\n",
    "        outputs = {\"answer\": answer},\n",
    "        dataset_id = ds1.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b2bc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import TextSplitter, TokenTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the document from web\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.langchain.com/langsmith\")\n",
    "raw_documents = api_loader.load()\n",
    "\n",
    "# Transform HTML raw data to clean text and split into chunks\n",
    "doc_transformer = Html2TextTransformer()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "text_splitter = TokenTextSplitter(model_name = \"gpt-4o\", chunk_size = 2000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(transformed)\n",
    "\n",
    "# Create a vector store retriever\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4054f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the prompt template that will be sent to the LLM.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangSmith's documentation.\"\n",
    "            \" LangChain is a framework for building applications using large language models.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\",\n",
    "        ),\n",
    "        (\"system\", \"{{context}}\"), \n",
    "        (\"human\", \"{question}\"),  \n",
    "    ]\n",
    ").partial(time=str(datetime.now()))\n",
    "\n",
    "# Initialize the LLM. We use a model with a large context window and low temperature for more factual responses.\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Define the generation chain. It pipes the prompt to the model and then to an output parser.\n",
    "rag_chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b33c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'whispered-taste-64' at:\n",
      "https://smith.langchain.com/o/bed0d9b9-d8f5-5181-ba80-dac89a730fec/datasets/a4b58ebc-7b6c-42af-9baf-a2d975e890c8/compare?selectedSessions=9b3060c0-5bfc-4702-a99b-68c11c7edcc0\n",
      "\n",
      "View all tests for Dataset unstruct_qa_evals at:\n",
      "https://smith.langchain.com/o/bed0d9b9-d8f5-5181-ba80-dac89a730fec/datasets/a4b58ebc-7b6c-42af-9baf-a2d975e890c8\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b7d559b8-b746-4d5e-a901-cc00c1de73d9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.035957</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.035365</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.889630</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.962794</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.035957</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.109120</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.182284</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.correctness error  execution_time  \\\n",
       "count                    2.0     0        2.000000   \n",
       "unique                   NaN     0             NaN   \n",
       "top                      NaN   NaN             NaN   \n",
       "freq                     NaN   NaN             NaN   \n",
       "mean                     1.0   NaN        4.035957   \n",
       "std                      0.0   NaN        3.035365   \n",
       "min                      1.0   NaN        1.889630   \n",
       "25%                      1.0   NaN        2.962794   \n",
       "50%                      1.0   NaN        4.035957   \n",
       "75%                      1.0   NaN        5.109120   \n",
       "max                      1.0   NaN        6.182284   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      2  \n",
       "unique                                     2  \n",
       "top     b7d559b8-b746-4d5e-a901-cc00c1de73d9  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'whispered-taste-64',\n",
       " 'results': {'c1a04913-6e28-439f-9795-d03cbb4f3fc6': {'input': {'question': \"What's a langsmith dataset?\"},\n",
       "   'feedback': [EvaluationResult(key='correctness', score=1, value='CORRECT', comment='CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4c81e3ac-94a9-41c7-9f32-ee44e2ab839f'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 6.182284,\n",
       "   'run_id': 'b7d559b8-b746-4d5e-a901-cc00c1de73d9',\n",
       "   'output': 'A LangSmith dataset is a collection of data used within the LangChain framework to facilitate the training, evaluation, and testing of language models. These datasets can include various types of data, such as text, images, or other media, and are used to improve the performance and accuracy of language models by providing them with relevant and diverse examples. LangSmith datasets are typically organized and managed to ensure they are easily accessible and usable for different stages of model development and deployment.',\n",
       "   'reference': {'answer': 'A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.'}},\n",
       "  '542fc116-8363-4734-93e6-07f4955ccf1f': {'input': {'question': 'What is LangChain?'},\n",
       "   'feedback': [EvaluationResult(key='correctness', score=1, value='CORRECT', comment='CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fec8706e-2cf6-498a-9a9a-8fe44592f656'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 1.88963,\n",
       "   'run_id': 'ab0d6b14-1550-4020-8cc1-5e93312b900e',\n",
       "   'output': 'LangChain is a framework designed to simplify the development of applications that utilize large language models (LLMs). It provides a suite of tools and components that help developers build applications with capabilities such as language understanding, generation, and interaction. LangChain is particularly useful for creating applications that require complex language processing tasks, enabling developers to focus on building their applications without needing to manage the intricacies of working directly with language models.',\n",
       "   'reference': {'answer': 'LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.'}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the evaluation to use the qa evaluator for grading correctness\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators = [\"qa\"]\n",
    ")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name = \"unstruct_qa_evals\", \n",
    "    llm_or_chain_factory = rag_chain, \n",
    "    evaluation = eval_config, \n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b5b24",
   "metadata": {},
   "source": [
    "#### Modifying the prompt and re-running the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b397628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the prompt template that will be sent to the LLM.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangSmith's documentation.\"\n",
    "            \" LangChain is a framework for building applications using large language models.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\",\n",
    "        ),\n",
    "        (\"system\", \"{{context}}\"), \n",
    "        (\"human\", \"{question}\"), \n",
    "        (\n",
    "            \"system\",\n",
    "            \"Respond as best as you can. if no documents are retrieved or if you cannot find an answer in the retrieved documents,\"\n",
    "            \"admit you do not know or that you can't find the supported results.\"\n",
    "        ) \n",
    "    ]\n",
    ").partial(time=str(datetime.now()))\n",
    "\n",
    "# Initialize the LLM. We use a model with a large context window and low temperature for more factual responses.\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Define the generation chain. It pipes the prompt to the model and then to an output parser.\n",
    "rag_chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34dfc629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'flowery-laugh-57' at:\n",
      "https://smith.langchain.com/o/bed0d9b9-d8f5-5181-ba80-dac89a730fec/datasets/a4b58ebc-7b6c-42af-9baf-a2d975e890c8/compare?selectedSessions=76959c59-be19-47e8-9b32-e6d788f8940c\n",
      "\n",
      "View all tests for Dataset unstruct_qa_evals at:\n",
      "https://smith.langchain.com/o/bed0d9b9-d8f5-5181-ba80-dac89a730fec/datasets/a4b58ebc-7b6c-42af-9baf-a2d975e890c8\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b778a328-691b-4a34-9ba7-81838d4cdd4d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.671546</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156374</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.560973</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.616259</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.671546</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.726832</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.782119</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.correctness error  execution_time  \\\n",
       "count               2.000000     0        2.000000   \n",
       "unique                   NaN     0             NaN   \n",
       "top                      NaN   NaN             NaN   \n",
       "freq                     NaN   NaN             NaN   \n",
       "mean                0.500000   NaN        2.671546   \n",
       "std                 0.707107   NaN        0.156374   \n",
       "min                 0.000000   NaN        2.560973   \n",
       "25%                 0.250000   NaN        2.616259   \n",
       "50%                 0.500000   NaN        2.671546   \n",
       "75%                 0.750000   NaN        2.726832   \n",
       "max                 1.000000   NaN        2.782119   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      2  \n",
       "unique                                     2  \n",
       "top     b778a328-691b-4a34-9ba7-81838d4cdd4d  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'flowery-laugh-57',\n",
       " 'results': {'c1a04913-6e28-439f-9795-d03cbb4f3fc6': {'input': {'question': \"What's a langsmith dataset?\"},\n",
       "   'feedback': [EvaluationResult(key='correctness', score=0, value='INCORRECT', comment='INCORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('402c9aba-d8d0-4288-b14b-d8e906a73e6e'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 2.782119,\n",
       "   'run_id': 'b778a328-691b-4a34-9ba7-81838d4cdd4d',\n",
       "   'output': 'I can\\'t find the specific details about a \"langsmith dataset\" in the provided documents. However, in the context of LangChain and similar frameworks, a dataset typically refers to a collection of data that can be used for training, testing, or evaluating language models. If \"langsmith dataset\" refers to a specific feature or concept within LangChain, I would need more detailed documentation to provide an accurate description.',\n",
       "   'reference': {'answer': 'A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.'}},\n",
       "  '542fc116-8363-4734-93e6-07f4955ccf1f': {'input': {'question': 'What is LangChain?'},\n",
       "   'feedback': [EvaluationResult(key='correctness', score=1, value='CORRECT', comment='CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1a05b286-edbb-4e8f-9828-a8c34b87363a'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 2.560973,\n",
       "   'run_id': '8c602da1-6ca3-41a6-b11e-cda7f6a0f290',\n",
       "   'output': 'LangChain is a framework designed for building applications that utilize large language models (LLMs). It provides tools and components to facilitate the development of applications that can leverage the capabilities of LLMs for various tasks, such as natural language processing, text generation, and more. LangChain aims to simplify the integration and use of language models in software applications, making it easier for developers to create sophisticated language-based functionalities.',\n",
       "   'reference': {'answer': 'LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.'}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the evaluation to use the qa evaluator for grading correctness\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators = [\"qa\"]\n",
    ")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name = \"unstruct_qa_evals\", \n",
    "    llm_or_chain_factory = rag_chain, \n",
    "    evaluation = eval_config, \n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3bc5e",
   "metadata": {},
   "source": [
    "### Structured Data Comparison Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "190c4032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='Contract Extraction Eval Data', description=None, data_type=<DataType.kv: 'kv'>, id=UUID('c8f2934d-a5a9-4900-bf59-7cc0f9bed759'), created_at=datetime.datetime(2025, 10, 16, 21, 6, 1, 771546, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 10, 16, 21, 6, 1, 771546, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a public dataset on LangSmith\n",
    "dataset_url = \"https:/smith.langchain.com/public/08ab7912-006e-4c00-a973-0f833e74907b/d\"\n",
    "dataset_name = \"Contract Extraction Eval Data\"\n",
    "\n",
    "# Clone the downloaded dataset\n",
    "client.clone_public_dataset(dataset_url, dataset_name = dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa655cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define the schema of party's address\n",
    "class Address(BaseModel):\n",
    "    street: str\n",
    "    city: str\n",
    "    state: str\n",
    "    \n",
    "# Define the schema of party in the contract\n",
    "class Party(BaseModel): \n",
    "    name: str\n",
    "    address: Address\n",
    "\n",
    "# Top-level schema for the entire contract\n",
    "class Contract(BaseModel):     \n",
    "    document_title: str\n",
    "    effective_date: str\n",
    "    parties: List[Party]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0be11887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\", max_tokens = 3000)\n",
    "\n",
    "extraction_chain = create_extraction_chain(Contract.model_json_schema(),llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "        evaluators = [\"json_edit_distance\"]\n",
    ")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name = \"Contract Extraction Eval Data\", \n",
    "    llm_or_chain_factory = extraction_chain,\n",
    "    evaluation = eval_config,\n",
    "    input_mapper=lambda x: {\"input\": x[\"context\"]},\n",
    "    output_mapper = lambda x: x[\"text\"],  \n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ddce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
